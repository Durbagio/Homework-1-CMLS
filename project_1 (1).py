# -*- coding: utf-8 -*-
"""Project 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kig4hiyLvqzEKjRXl_CvZLYicVXEvYmE

# Project 1: Audio Effects

# General Discussion

### Importing libraries and database
"""

import librosa
import numpy as np
import os
import matplotlib.pyplot as plt
import sklearn.svm
import IPython
import IPython.display as ipd
import scipy as sp
import multiprocessing as mp

"""Remember to create a shortcut in your drive to the shared "Project 1" folder"""

from google.colab import drive
drive.mount('/content/drive')

"""Dataset cardinality"""

classes = ['NoFX', 'Distortion', 'Tremolo']

for c in classes:
    root='/content/drive/My Drive/Project 1/Dataset/Gitarre polyphon/Samples/{}'.format(c)
    class_files = [f for f in os.listdir(root) if f.endswith('.wav')]
    print('Guitar poly, {},'.format(c), len(class_files))
    root='/content/drive/My Drive/Project 1/Dataset/Gitarre monophon/Samples/{}'.format(c)
    class_files = [f for f in os.listdir(root) if f.endswith('.wav')]
    print('Guitar mono, {},'.format(c), len(class_files))
    root='/content/drive/My Drive/Project 1/Dataset/Bass monophon/Samples/{}'.format(c)
    class_files = [f for f in os.listdir(root) if f.endswith('.wav')]
    print('Bass mono, {},'.format(c), len(class_files))

"""### Feature Computation
Spectral Rolloff, Spectral RMS, Zero Crossing Rate, Spectral Centroid, Spectral Bandwidth, Spectral Contrast, Spectral Flatness, Harmonic RMS, Hi-End RMS, Mel Spectrogram, Mel-frequency Cepstral Coefficients (14).
"""

def harmonic_rms (signal):
    # Extract the harmonics
    harmonics = librosa.effects.harmonic(signal)
    # Calculate the rms of the harmonics only
    rms_value = np.mean(librosa.feature.rms(harmonics))

    return rms_value

def hi_end_rms (signal, Fs):

    # Setup window values and allocate array
    win_length = int(np.floor(0.01 * Fs))
    hop_size = int(np.floor(0.0075 * Fs))
    window = sp.signal.get_window(window='hanning', Nx=win_length)
    win_number = int(np.floor((signal.shape[0] - win_length) / hop_size))

    rms_frame = np.zeros((win_number))

    for i in np.arange(win_number):
        # Compute the frame
        frame = signal[i * hop_size : i * hop_size + win_length]
        frame_wind = frame * window
    
        # Compute the spectrogram of this frame
        spec = np.fft.fft(frame_wind)
        nyquist = int(np.floor(spec.shape[0] / 2))
        spec = spec[1:nyquist]

        # Design the high pass
        filt = sp.signal.butter(2, 1200, fs=Fs, btype='highpass', output='sos')
        # Filter the signal
        hi_end = sp.signal.sosfilt(filt, spec)
        # Compute the rms value
        rms_frame = librosa.feature.rms(y=None, S=spec)
    
    # Average the value of whole signal
    rms_hi_end = np.mean(rms_frame)

    return rms_hi_end

classes = ['NoFX', 'Distortion', 'Tremolo']
class_features={'NoFX':[],'Distortion':[],'Tremolo':[]}

frame_len= 8192
win_len=2048 
hop_len=512

# Number of Mel-frequency cepstral coefficients
n_mfcc = 14 
n_features = 10+n_mfcc

feature_names = ['Spectral Rolloff','RMS','ZCR','Spectral Centroid', 'Bandwidth', 'Spectral Contrast', 'Spectral Flatness', 'Harmonic RMS', 'High End RMS', 'MEL Spectrogram', 'MFCC1','MFCC2','MFCC3','MFCC4','MFCC5','MFCC6','MFCC7','MFCC8','MFCC9','MFCC10','MFCC11','MFCC12','MFCC13','MFCC14']

for c in classes:
    # Find the folder (select between Guitar monophonic, Guitar poliphonic, Bass monophonic)
    root='/content/drive/My Drive/Project 1/Dataset/Gitarre monophon/Samples/{}'.format(c)
    # root='/content/drive/My Drive/Project 1/Dataset/Gitarre polyphon/Samples/{}'.format(c)
    # root='/content/drive/My Drive/Project 1/Dataset/Bass monophon/Samples/{}'.format(c)
    
    # List the files
    class_files = [f for f in os.listdir(root) if f.endswith('.wav')]
    class_files = class_files[0: 600]

    # Declare the number of feature and files and allocate the array
    n_files = len(class_files)

    features = np.zeros((n_files, n_features))

    for index, f in enumerate(class_files):
        # Load the file
        audio, fs = librosa.load(os.path.join(root, f), sr=None)

        # Compute the features, take the mean value on all the frames, save them in the features array
        features[index, 0] = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=fs, S=None, n_fft=win_len, hop_length=hop_len))
        features[index, 1] = np.mean(librosa.feature.rms(y=audio, S=None, frame_length=win_len, hop_length=hop_len))
        features[index, 2] = np.mean(librosa.feature.zero_crossing_rate(audio, frame_length=win_len, hop_length=hop_len))
        features[index, 3] = np.mean(librosa.feature.spectral_centroid(y = audio,sr = fs,S = None,n_fft = win_len, hop_length = hop_len))
        features[index, 4] = np.mean(librosa.feature.spectral_bandwidth(y = audio,sr = fs,S = None,n_fft = win_len, hop_length = hop_len))
        features[index, 5] = np.mean(librosa.feature.spectral_contrast(y = audio,sr = fs,S = None,n_fft = win_len, hop_length = hop_len))
        features[index, 6] = np.mean(librosa.feature.spectral_flatness(y = audio, S = None,n_fft = win_len, hop_length = hop_len))
        features[index, 7] = harmonic_rms(audio)
        features[index, 8] = hi_end_rms(audio, fs)
        features[index, 9] = np.mean(librosa.feature.melspectrogram(y = audio, sr = fs, S = None,n_fft = win_len, hop_length = hop_len))
        features[index, 10:] = np.mean(librosa.feature.mfcc(y=audio, sr=fs, S=None, n_mfcc=n_mfcc, dct_type=2, norm='ortho'), axis=1)

    # Store features in the class features array
    class_features[c] = features
    print("Ok for class {}".format(c))

"""### Implementing the model

DB
"""

# Declaring the features matrix of each class (X) and its label (Y)

# No effect
X_noFX = class_features['NoFX']
y_noFX = np.zeros((X_noFX.shape[0],))

# Distortion
X_Dist = class_features['Distortion']
y_Dist = np.ones((X_Dist.shape[0],))

# Tremolo
X_Trem = class_features['Tremolo']
y_Trem = np.ones((X_Trem.shape[0],))*2

# Concatenate all (features and labels)
X = np.concatenate((X_noFX, X_Dist, X_Trem), axis=0)
Y = np.concatenate((y_noFX, y_Dist, y_Trem), axis = 0)

"""#### SVC"""

from sklearn.preprocessing import Normalizer
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_predict, StratifiedKFold, cross_val_score
from sklearn.feature_selection import  SelectKBest
from sklearn.metrics import classification_report, confusion_matrix

"""All features"""

# Normalization
scaler = Normalizer().fit(X)
X_norm = scaler.transform(X)

# Define the cv split
split = StratifiedKFold(n_splits=5, shuffle=True)

# Define the model
svm = SVC(C = 20, kernel = 'rbf', probability = True)

# Train and predict
svc_prediction = cross_val_predict(estimator = svm, X = X, y = Y, cv=split)

# Print the results
classes1 = {'NoFX':0,'Distortion':1,'Tremolo':2}
print(classification_report(Y, svc_prediction, target_names = classes1.keys()))

#Confusion matrix (SVM, all features)
confusion_matrix(Y, svc_prediction)

"""Feature Selection"""

#Normalization
scaler = Normalizer().fit(X)
X_norm = scaler.transform(X)

#Define the cv split
split = StratifiedKFold(n_splits=5, shuffle=True)

#Define the model
svm = SVC(C = 20, kernel = 'rbf', probability = True)

#Selection of k best features:
n_range = range(1, n_features)
n_scores = []

for i in n_range:
    selector = SelectKBest(k = i)
    X_selection = selector.fit_transform(X_norm, Y)

    scores = cross_val_score(svm, X=X_selection, y=Y, cv=split, scoring='accuracy')
    n_scores.append(scores.mean())

# Plot accuracy vs K features selected
plt.plot(n_range, n_scores)
plt.xlabel('K for Selecting K features')
plt.ylabel('Cross-Validated Accuracy')

#Normalization
scaler = Normalizer().fit(X)
X_norm = scaler.transform(X)

#Define the CV split
split = StratifiedKFold(n_splits=5, shuffle=True)

#Feature selection
selector = SelectKBest(k = 5)
X_selection = selector.fit_transform(X_norm, Y)

#Defining the model
svm = SVC(C = 20, kernel = 'rbf', probability = True)

#Train and predict
svc_prediction = cross_val_predict(estimator = svm, X = X_selection, y = Y, cv=split)

#Print the results
classes1 = {'NoFX':0,'Distortion':1,'Tremolo':2}
print(classification_report(Y, svc_prediction, target_names = classes1.keys()))


mask = selector.get_support()
new_features = []

for bool, feature in zip(mask, feature_names):
    if bool:
        new_features.append(feature)

print('Best features are:', new_features)

#Confusion matrix 
confusion_matrix(Y, svc_prediction)

"""#### K-Neighbors
All features
"""

from sklearn.neighbors import KNeighborsClassifier

# Define the model
split = StratifiedKFold(n_splits=5, shuffle=True)

# Select number of neighbors (k)
n_range = range(1,31)
n_scores = []

for n in n_range: 
    knn = KNeighborsClassifier(n_neighbors=n, weights='distance') 

    scores = cross_val_score(knn, X=X, y=Y, cv=split, scoring='accuracy')
    n_scores.append(scores.mean())

# Plot accuracy vs K neighbors selected
plt.plot(n_range, n_scores)
plt.xlabel('Value of K for KNN')
plt.ylabel('Cross-Validated Accuracy')

# Define the model
model = KNeighborsClassifier(n_neighbors=4, weights='distance')
split = StratifiedKFold(n_splits=5, shuffle=True)

# Train and predict
knn_prediction = cross_val_predict(estimator=model, X=X, y=Y, cv=split)

# Print the results
classes1 = {'NoFX':0,'Distortion':1,'Tremolo':2}
print(classification_report(Y, knn_prediction, target_names = classes1.keys()))

# Confusion matrix
confusion_matrix(Y, knn_prediction)

"""Feature selection"""

# Normalization
scaler = Normalizer().fit(X)
X_norm = scaler.transform(X)

# Define the CV split
split = StratifiedKFold(n_splits=5, shuffle=True)

# Define the model
model = KNeighborsClassifier(n_neighbors=4, weights='distance')

# Selection of K best features:
n_range = range(1, n_features)
n_scores = []

for i in n_range:
    selector = SelectKBest(k = i)
    X_selection = selector.fit_transform(X_norm, Y)

    scores = cross_val_score(model, X=X_selection, y=Y, cv=split, scoring='accuracy')
    n_scores.append(scores.mean())

# Plot accuracy vs K features selected
plt.plot(n_range, n_scores)
plt.xlabel('K for Selecting K features')
plt.ylabel('Cross-Validated Accuracy')

# Selection of K best features:
selector = SelectKBest(k = 5)
X_selection = selector.fit_transform(X_norm, Y)

# Define the model
model = KNeighborsClassifier(n_neighbors=4, weights='distance')
split = StratifiedKFold(n_splits=5, shuffle=True)

# Train and predict
knn_prediction = cross_val_predict(estimator=model, X=X_selection, y=Y, cv=split)

# Print the results
classes1 = {'NoFX':0,'Distortion':1,'Tremolo':2}
print(classification_report(Y, knn_prediction, target_names = classes1.keys()))

mask = selector.get_support()
new_features = []

for bool, feature in zip(mask, feature_names):
    if bool:
        new_features.append(feature)

print('Best features are:', new_features)

# Confusion matrix
confusion_matrix(Y, knn_prediction)

"""#### Best features computation"""

# Scoring
slct = SelectKBest(k="all")
slct.fit(X_norm, Y)
scores = slct.scores_

# Sort features by score from largest to smallest
named_scores = zip(feature_names, scores)
sorted_named_scores = sorted(named_scores, key=lambda z: z[1], reverse=True)
 
sorted_scores = [each[1] for each in sorted_named_scores]
sorted_names = [each[0] for each in sorted_named_scores]
 
y_pos = np.arange(len(feature_names))# drawing order from top to bottom

# Plot
fig, ax = plt.subplots()
ax.barh(y_pos, sorted_scores, height=0.7, align='center', color='#AAAAAA', tick_label=sorted_names)
ax.set_yticks(y_pos)
ax.set_xlabel('Feature Score')
ax.set_ylabel('Feature Name')
ax.invert_yaxis()
ax.set_title('F_classif scores (in SelectKBest) of the features.')

# Add the value for every line
for score, pos in zip(sorted_scores, y_pos):
        ax.text(score + 20, pos, '%.1f' % score, ha='center', va='bottom', fontsize=8)